{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a24574f-810d-48f5-b6cf-92297cd21e30",
   "metadata": {},
   "source": [
    "# Lab-3-Chatbot&RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247ed1a-00a6-4005-b362-8dcb7c0eaa19",
   "metadata": {},
   "source": [
    "## 1. Basic Completion and Chat\n",
    "### Download Phi-3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18203a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub as hf_hub\n",
    "from pathlib import Path\n",
    "\n",
    "llm_model_id = \"OpenVINO/Phi-3-mini-4k-instruct-int4-ov\"\n",
    "llm_model_path = \"model/phi-3-mini-4k-instruct-ov\"\n",
    "\n",
    "if not Path(llm_model_path).exists():\n",
    "    hf_hub.snapshot_download(llm_model_id, local_dir=llm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928626d",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a75c915-d4fc-4016-89c6-dba61edea9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to GPU ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openvino import OpenVINOLLM\n",
    "\n",
    "ov_config = {\n",
    "    \"PERFORMANCE_HINT\": \"LATENCY\",\n",
    "    \"NUM_STREAMS\": \"1\",\n",
    "    \"CACHE_DIR\": \"\",\n",
    "}\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    return f\"<|system|><|end|><|user|>{completion}<|end|><|assistant|>\\n\"\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|system|>{message.content}<|end|>\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|user|>{message.content}<|end|>\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|assistant|>{message.content}<|end|>\"\n",
    "\n",
    "    # ensure we start with a system prompt, insert blank if needed\n",
    "    if not prompt.startswith(\"<|system|>\"):\n",
    "        prompt = \"<|system|><|end|>\" + prompt\n",
    "\n",
    "    # add final assistant prompt\n",
    "    prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "ov_llm = OpenVINOLLM(\n",
    "    model_id_or_path=llm_model_path,\n",
    "    context_window=3900,\n",
    "    max_new_tokens=1024,\n",
    "    model_kwargs={\"ov_config\": ov_config},\n",
    "    generate_kwargs={\"pad_token_id\": 32000, \"do_sample\": False, \"temperature\": None, \"top_p\": None},\n",
    "    stopping_ids=[32000],\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    device_map=\"gpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51204e94-b05c-4a86-b057-55614d017c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stop_tokens = [\"<|endoftext|>\"]\n",
    "stop_tokens = ov_llm._tokenizer.convert_tokens_to_ids(stop_tokens)\n",
    "stop_tokens = [StopOnTokens(stop_tokens)]\n",
    "ov_llm._stopping_criteria = StoppingCriteriaList(stop_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6719d-299c-4a11-9f0a-b2ef2d5db716",
   "metadata": {},
   "source": [
    "### Call complete with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9aec4ad-efd1-4a56-9343-e8d7cc61ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO (Open Visual Inference and Object Detection) is an open-source toolkit developed by Intel, designed to optimize and deploy deep learning models on Intel hardware. It provides a comprehensive set of tools and libraries to help developers and researchers accelerate the inference of deep learning models on Intel's hardware, such as CPUs, GPUs, and FPGAs, with a focus on Intel'sinetworks.\n",
      "\n",
      "The main goal of OpenVINO is to make it easier for developers to deploy their trained deep learning models on Intel hardware, without the need for extensive knowledge of deep learning and inference optimization techniques. OpenVINO achieves this by providing a standardized interface (Inference Engine) for loading, optimizing, and running deep learning models.\n",
      "\n",
      "Some key features of OpenVINO include:\n",
      "\n",
      "1. Model Optimization: OpenVINO provides a set of tools to optimize deep learning models for Intel hardware, including AutoDevices, AutoOptimizers, and AutoModifier. These tools automatically detect and optimize the model's dependencies on specific hardware components, such as CPUs, GPUs, and FPGAs, to achieve the best possible performance.\n",
      "\n",
      "2. Model Loading and Interpretation: OpenVINO provides a standardized interface (Inference Engine) to load and interpret deep learning models in various formats, such as ONNX, CoreML, and OpenVDB. This allows developers to easily integrate their models with OpenVINO's optimization and deployment tools.\n",
      "\n",
      "3. Inference Acceleration: OpenVINO accelerates deep learning inference on Intel hardware by leveraging the Intel Deep Learning SDK (Intel® oneAPI), which provides optimized libraries and runtime support for deep learning and computer vision tasks.\n",
      "\n",
      "4. Model Serving: OpenVINO supports serving optimized models through Intel's distribution platforms, such as Intel® Distribution for OpenVINO, which provides pre-built optimized models for popular deep learning frameworks like TensorFlow, PyTorch, and MXNet.\n",
      "\n",
      "5. Community and Support: OpenVINO has a strong community of developers and researchers, and Intel provides extensive documentation, tutorials, and support resources to help users get started with OpenVINO and optimize their deep learning models for Intel hardware.\n",
      "\n",
      "In summary, OpenVINO is an open-source toolkit developed by Intel to help developers and researchers accelerate the deployment of deep learning models on Intel hardware, with a focus on CPUs, GPUs, and FPGAs. It provides a standardized interface for loading, optimizing, and running deep learning models, as well as tools to accelerate inference and serve optimized models through Intel's distribution platforms."
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "response = ov_llm.stream_complete(\"What is OpenVINO ?\")\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb5a76-e92e-472a-aa2c-313e71c08338",
   "metadata": {},
   "source": [
    "### Manage chat history by Chat Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d1b0b9-109a-42fe-96d3-c7d1ba074737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of weather's whimsical play,\n",
      "Where the sky'rows with a furrowed brow,\n",
      "A curious tale unfolds today,\n",
      "Of paws and tails in a watery vow.\n",
      "\n",
      "Raindrops patter, a gentle tune,\n",
      "On rooftops, they dance and sway,\n",
      "But lo! A twist in the afternoon,\n",
      "As cats and dogs in rain do stray.\n",
      "\n",
      "From the heavens, they descend,\n",
      "In a downpour, a curious sight,\n",
      "A symphony of paws and tails,\n",
      "In the rain's embrace, they take flight.\n",
      "\n",
      "The streets awash with a watery sheen,\n",
      "A spectacle, both bizarre and grand,\n",
      "As felines and canines, once unseen,\n",
      "Now roam the land with a paw in hand.\n",
      "\n",
      "But fear not, dear friend, for this is but a jest,\n",
      "A whimsical tale, a playful rhyme,\n",
      "For in the realm of weather's behest,\n",
      "Raining cats and dogs are but a mime.\n",
      "\n",
      "So let us laugh, and let us cheer,\n",
      "For the sky's a canvas, vast and wide,\n",
      "Where imagination's rain can appear,\n",
      "And in our hearts, it shall forever reside."
     ]
    }
   ],
   "source": [
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "chat_engine = SimpleChatEngine.from_defaults(llm=ov_llm)\n",
    "\n",
    "response = chat_engine.stream_chat(\n",
    "    \"Write me a poem about raining cats and dogs.\"\n",
    ")\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9484a79b-fae3-4e80-b17e-d37a563aed6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: \"The Paws and Puddles Parade\"\n",
      "\n",
      "This poem is titled \"The Paws and Puddles Parade\" to reflect the playful and whimsical nature of the imagery, as well as the unexpected parade of cats and dogs in the rain. The title also hints at the lightheartedness and joy that can be found in such a peculiar scenario, inviting readers to embrace the absurdity and enjoy the creative freedom of the poem."
     ]
    }
   ],
   "source": [
    "response = chat_engine.stream_chat(\n",
    "    \"name this poem\"\n",
    ")\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c6cdf6-4358-43aa-a380-2c315f36c228",
   "metadata": {},
   "source": [
    "## 2. Basic RAG (Vector Search, Summarization)\n",
    "### Export Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "536ea7ff-033b-4a53-ba2c-6169c69e47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "embedding_model_path = \"bge-small-en-v1.5-ov\"\n",
    "\n",
    "if not Path(embedding_model_path).exists():\n",
    "    !optimum-cli export openvino --model {embedding_model_id} --task feature-extraction {embedding_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f82691",
   "metadata": {},
   "source": [
    "### Initialize Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7a768ac-b701-4722-8d88-f1d2561feb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_openvino import OpenVINOEmbedding\n",
    "\n",
    "ov_embedding = OpenVINOEmbedding(model_id_or_path=embedding_model_path, device=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ace02-5bec-4815-a709-0b6423497516",
   "metadata": {},
   "source": [
    "### Basic RAG (Vector Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed13be16-0364-445e-acc9-d0b466411d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "Settings.embed_model = ov_embedding\n",
    "Settings.llm = ov_llm\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"Product Brief.txt\"]\n",
    ")\n",
    "documents = reader.load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b492fb4-7667-492b-b8f8-0048fc2b592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of cores per socket in an Intel Xeon 6 processor is up to 144 cores."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\"what's the maximum number of cores per socket in an Intel Xeon 6 processor\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1931c07-e568-4586-b5e7-885f450b9082",
   "metadata": {},
   "source": [
    "### Basic RAG (Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edcd0e55-3513-49d2-b974-6e0374a8be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "\n",
    "summary_index = SummaryIndex.from_documents(documents)\n",
    "summary_engine = summary_index.as_query_engine(streaming=True, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25d580db-0a57-484a-b558-414b51b807a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Intel® Xeon® 6 processors with Efficient-cores (E-cores) are designed to meet the demands of network and edge scalability, offering high performance per watt and up to 144 cores per socket. These processors provide greater parallel processing for workload throughput, maximizing value. They are built with integrated accelerators to enhance efficiency and productivity for network and security workloads. The E-cores support next-gen security capabilities, such as deep packet inspection and zero trust network, and are equipped with Intel® Trust Domain Extensions (Intel® TDX) and Intel® Software Guard Extensions (Intel® SGX) for secure in-flight applications and data. The processors also offer longevity, simplified designs, and longer deployments, with support from Intel® Ethernet, GPU, IPU, and an ecosystem of optimized software and tools. These features make Intel® Xeon® 6 processors with E-cores ideal for software engineers and network architects working on edge solutions, 5G networks, and edge-to-cloud infrastructure."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\"Can you summerize this document ?\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b04a03-7906-4fa0-ba1d-1419bd0c94d6",
   "metadata": {},
   "source": [
    "## 3. Advanced RAG (Routing)\n",
    "### Build a Router that can choose whether to do vector search or summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a94d6747-8c41-4d05-b011-082b554088f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "vector_tool = QueryEngineTool(\n",
    "    index.as_query_engine(streaming=True),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"vector_search\",\n",
    "        description=\"Useful for searching for basic facts about Intel Xeon 6 processors\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool(\n",
    "    index.as_query_engine(streaming=True, response_mode=\"tree_summarize\"),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"summary\",\n",
    "        description=\"Useful for summarizing an entire document of Intel Xeon 6 processors\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a86ec58-15e1-4e1e-b761-7e3ea7bc9b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The first choice is most relevant because it pertains to providing basic facts about Intel Xeon 6 processors, which would likely include specifications such as the maximum number of cores per socket..\n",
      "\u001b[0mThe maximum number of cores per socket in an Intel Xeon 6 processor is up to 144 cores."
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "\n",
    "query_engine = RouterQueryEngine.from_defaults(\n",
    "    [vector_tool, summary_tool], select_multi=False, verbose=True, llm=ov_llm\n",
    ")\n",
    "\n",
    "streaming_response = query_engine.query(\n",
    "    \"what's the maximum number of cores per socket in an Intel Xeon 6 processor\"\n",
    ")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4526ea68-d594-4c57-9a42-b028e315aa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question 'Can you summarize this document?' directly relates to the ability to provide a concise overview of an entire document, which aligns with choice (2)..\n",
      "\u001b[0mThe Intel® Xeon® 6 processors with Efficient-cores (E-cores) are designed to meet the demands of network and edge scalability, offering high performance per watt and up to 144 cores per socket. These processors provide greater parallel processing capabilities, enhancing workload throughput and value. They are optimized for networking, edge, and security workloads, with integrated accelerators for improved efficiency and productivity. The E-cores support next-gen security features like deep packet inspection and zero trust networks, and utilize confidential computing technologies to secure data. The processors offer longevity and simplified designs, with PCH-less boot and compatibility with P-core processors, ensuring businesses can capitalize on their investments. Overall, Intel® Xeon® 6 processors with E-cores are ideal for software engineers and network architects seeking to boost edge solutions, enhance security, and improve sustainability efforts."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"Can you summerize this document ?\"\n",
    ")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f757fe7",
   "metadata": {},
   "source": [
    "## 4. Agentic RAG\n",
    "\n",
    "### Build tools of calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a444187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "\n",
    "\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "divide_tool = FunctionTool.from_defaults(fn=divide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b868cc",
   "metadata": {},
   "source": [
    "### Create an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76419c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools([multiply_tool, divide_tool, vector_tool], llm=ov_llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a86daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 42aef0c2-4b73-4285-8af1-92d7c4408bb5. Step input: What's the maximum number of cores of 6 sockets of Intel Xeon 6 processors ? Go step by step, using a tool to do any math.\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: vector_search\n",
      "Action Input: {'input': 'maximum number of cores in Intel Xeon 6 processors'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: The maximum number of cores in Intel Xeon 6 processors is up to 144 cores per socket.\n",
      "\u001b[0m> Running step ad29b7e9-e8df-496a-8a4d-f2ffb91b054b. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: The maximum number of cores for an Intel Xeon 6 processor with 6 sockets is 864 cores (144 cores per socket multiplied by 6 sockets).\n",
      "\n",
      "To calculate this, we can use the multiply tool:\n",
      "\n",
      "Thought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Action: multiply\n",
      "Action Input: {'a': 144, 'b': 6}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 864\n",
      "\u001b[0m> Running step 320bd891-594a-4f1a-926f-2caebe9b866a. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: The maximum number of cores for an Intel Xeon 6 processor with 6 sockets is 864 cores.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What's the maximum number of cores of 6 sockets of Intel Xeon 6 processors ? Go step by step, using a tool to do any math.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de0af422",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
