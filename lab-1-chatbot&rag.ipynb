{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a24574f-810d-48f5-b6cf-92297cd21e30",
   "metadata": {},
   "source": [
    "# Lab-1-Chatbot&RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247ed1a-00a6-4005-b362-8dcb7c0eaa19",
   "metadata": {},
   "source": [
    "## 1. Basic Completion and Chat\n",
    "### Downlord Phi-3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18203a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub as hf_hub\n",
    "from pathlib import Path\n",
    "\n",
    "llm_model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "llm_model_path = \"Phi-3-mini-4k-instruct-ov\"\n",
    "\n",
    "if not Path(llm_model_path).exists():\n",
    "    hf_hub.snapshot_download(llm_model_id, local_dir=llm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928626d",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a75c915-d4fc-4016-89c6-dba61edea9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openvino import OpenVINOLLM\n",
    "\n",
    "ov_config = {\n",
    "    \"PERFORMANCE_HINT\": \"LATENCY\",\n",
    "    \"NUM_STREAMS\": \"1\",\n",
    "    \"CACHE_DIR\": \"\",\n",
    "}\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|system|>{message.content}<|end|>\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|user|>{message.content}<|end|>\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|assistant|>{message.content}<|end|>\"\n",
    "\n",
    "    # ensure we start with a system prompt, insert blank if needed\n",
    "    if not prompt.startswith(\"<|system|>\"):\n",
    "        prompt = \"<|system|><|end|>\" + prompt\n",
    "\n",
    "    # add final assistant prompt\n",
    "    prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    return f\"<|system|><|end|><|user|>{completion}<|end|><|assistant|>\\n\"\n",
    "\n",
    "ov_llm = OpenVINOLLM(\n",
    "    model_id_or_path=llm_model_path,\n",
    "    context_window=3900,\n",
    "    max_new_tokens=1024,\n",
    "    model_kwargs={\"ov_config\": ov_config},\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    device_map=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51204e94-b05c-4a86-b057-55614d017c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stop_tokens = [\"<|endoftext|>\"]\n",
    "stop_tokens = ov_llm._tokenizer.convert_tokens_to_ids(stop_tokens)\n",
    "stop_tokens = [StopOnTokens(stop_tokens)]\n",
    "ov_llm._stopping_criteria = StoppingCriteriaList(stop_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6719d-299c-4a11-9f0a-b2ef2d5db716",
   "metadata": {},
   "source": [
    "### Call complete with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a9aec4ad-efd1-4a56-9343-e8d7cc61ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "OpenVINO is an open-source toolkit developed by Intel that enables developers to optimize and deploy machine learning models for inference on Intel hardware. It provides a comprehensive set of tools and libraries to streamline the process of converting trained models into optimized formats, deploying them on Intel hardware, and integrating them into applications.\n",
      "\n",
      "OpenVINO offers a wide range of functionalities, including model conversion, optimization, and deployment, making it easier for developers to leverage the power of machine learning in their applications. It supports various model formats, including TensorFlow, Caffe, MXNet, and more, enseczing developers with the flexibility to work with different frameworks.\n",
      "\n",
      "The toolkit is designed to accelerate inference on Intel hardware, such as Intel® Data Center GPUs (DGPUs) and Xeon® processors with integrated GPUs, by optimizing the models for efficient execution. This optimization process involves techniques like model pruning, precision reduction, and kernel fusion, which help improve the performance and efficiency of the models.\n",
      "\n",
      "OpenVINO also provides a seamless integration with popular machine learning frameworks, allowing developers to easily convert and optimize their models without leaving their preferred environment"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "resp = ov_llm.stream_complete(\"What is OpenVINO ?\")\n",
    "\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06421eb-944b-4b2b-be84-cee0f54e151a",
   "metadata": {},
   "source": [
    "### Call chat with a list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f6fbc10-ec44-4522-8008-f164d61621fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO (Open Vision Inference Optimization Toolkit) is a platform developed by Intel, designed to optimize and deploy deep learning models for inference on Intel hardware. It provides a comprehensive set of tools and techniques to convert, optimize, and deploy deep learning models, particularly for Intel'enhanced hardware like the Intel® Xeon Phi™ coprocessor and the Movidius™ Myriad X Vision Processing Unit (VPU).\n",
      "\n",
      "OpenVINO offers several key features:\n",
      "\n",
      "1. Model Optimization: It provides tools to convert and optimize deep learning models from various frameworks (e.g., TensorFlow, Caffe, PyTorch, MATLAB) into an intermediate representation called the Open Model Zoo (OMZ) format. This format is optimized for inference on Intel hardware.\n",
      "\n",
      "2. Model Quantization: OpenVINO supports model quantization, which reduces the precision of the model's weights and activations, resulting in faster inference and lower memory usage.\n",
      "\n",
      "3. Model Fusion: OpenVINO can fuse multiple operations in a model into a single operation, reducing the computational complexity and improving inference speed.\n",
      "\n",
      "4"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are Kendrick.\"),\n",
    "    ChatMessage(role=\"user\", content=\"Write a verse.\"),\n",
    "]\n",
    "\n",
    "resp = ov_llm.stream_chat(messages)\n",
    "\n",
    "for token in resp.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c6cdf6-4358-43aa-a380-2c315f36c228",
   "metadata": {},
   "source": [
    "## Basic RAG (Vector Search, Summarization)\n",
    "### Export Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "536ea7ff-033b-4a53-ba2c-6169c69e47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "embedding_model_path = \"bge-small-en-v1.5-ov\"\n",
    "\n",
    "if not Path(embedding_model_path).exists():\n",
    "    !optimum-cli export openvino --model {embedding_model_id} --task feature-extraction {embedding_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f82691",
   "metadata": {},
   "source": [
    "### Initialize Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7a768ac-b701-4722-8d88-f1d2561feb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_openvino import OpenVINOEmbedding\n",
    "\n",
    "ov_embedding = OpenVINOEmbedding(model_id_or_path=embedding_model_path, device=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f16fef80-9fd7-4960-9f0d-936f94a27fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import io\n",
    "\n",
    "text_example_en_path = Path(\"text_example_en.pdf\")\n",
    "text_example_en = \"https://github.com/user-attachments/files/16171326/xeon6-e-cores-network-and-edge-brief.pdf\"\n",
    "\n",
    "if not text_example_en_path.exists():\n",
    "    r = requests.get(url=text_example_en)\n",
    "    content = io.BytesIO(r.content)\n",
    "    with open(\"text_example_en.pdf\", \"wb\") as f:\n",
    "        f.write(content.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ace02-5bec-4815-a709-0b6423497516",
   "metadata": {},
   "source": [
    "### Basic RAG (Vector Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ed13be16-0364-445e-acc9-d0b466411d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "\n",
    "Settings.embed_model = ov_embedding\n",
    "Settings.llm = ov_llm\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=text_example_en_path)\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b492fb4-7667-492b-b8f8-0048fc2b592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of cores per socket in an Intel Xeon 6 processor is 144."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\"what's the maximum number of cores per socket in an Intel Xeon 6 processor\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1931c07-e568-4586-b5e7-885f450b9082",
   "metadata": {},
   "source": [
    "### Basic RAG (Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "edcd0e55-3513-49d2-b974-6e0374a8be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "\n",
    "summary_index = SummaryIndex.from_documents(documents)\n",
    "summary_engine = summary_index.as_query_engine(streaming=True, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "25d580db-0a57-484a-b558-414b51b807a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of cores per socket in an Intel Xeon 6 processor is 144."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\"Can you summerize this document ?\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b04a03-7906-4fa0-ba1d-1419bd0c94d6",
   "metadata": {},
   "source": [
    "## Advanced RAG (Routing)\n",
    "### Build a Router that can choose whether to do vector search or summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a94d6747-8c41-4d05-b011-082b554088f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "vector_tool = QueryEngineTool(\n",
    "    index.as_query_engine(streaming=True),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"vector_search\",\n",
    "        description=\"Useful for searching for basic facts about Intel Xeon 6 processors\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool(\n",
    "    index.as_query_engine(streaming=True, response_mode=\"tree_summarize\"),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"summary\",\n",
    "        description=\"Useful for summarizing an entire document of Intel Xeon 6 processors\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a86ec58-15e1-4e1e-b761-7e3ea7bc9b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for a specific fact about the Intel Xeon 6 processors with E-cores, which is more likely to be found in a concise summary rather than a comprehensive document..\n",
      "\u001b[0mThe maximum number of cores per socket in an Intel Xeon 6 processor is 144."
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "\n",
    "query_engine = RouterQueryEngine.from_defaults(\n",
    "    [vector_tool, summary_tool], select_multi=False, verbose=True, llm=ov_llm\n",
    ")\n",
    "\n",
    "streaming_response = query_engine.query(\n",
    "    \"what's the maximum number of cores per socket in an Intel Xeon 6 processor\"\n",
    ")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4526ea68-d594-4c57-9a42-b028e315aa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question asks for a summary of the entire document, which aligns with choice 2 that is described as useful for summarizing an entire document..\n",
      "\u001b[0mThis document provides information on Intel® Xeon® 6 Processors with Efficient-cores, specifically the Intel® Xeon® 6780E, 6766E, 6756E, 6746E, 6740E, 6731E, and 6710E processors. It details various features, including the number of cores, base and turbo frequencies, cache size, TDP, maximum scalability, DDR5 8Ch memory support, Intel® UPI Links, Intel® DSA Devices, Intel® IAA Devices, Intel® QA Devices, Intel® DLB Devices, and Intel® QuickAssist Technology.\n",
      "\n",
      "The document also mentions that Intel® Xeon® 6 Processors support Intel® Speed Select Technology Performance Profile (Intel® SST-PP) and Intel® Turbo Boost Technology. It emphasizes that processor numbers are not a measure of performance and that the frequency of cores and core types can vary by workload, power consumption, and other factors.\n",
      "\n",
      "Additionally, the document notes that Intel does not commit or guarantee product availability"
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"Can you summerize this document ?\"\n",
    ")\n",
    "streaming_response.print_response_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
