{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291f5de2",
   "metadata": {},
   "source": [
    "# Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ad753",
   "metadata": {},
   "source": [
    "### Download Phi-3-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85e2fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82fff78a7df49eaae61ffb7964bf459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18181d8901045c4a08760976f3e1c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377d353b2c85475b98582ed835f540ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/921 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54377fc52434ca49aa0d7f6fb4fbfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77a3677433b431781445d41ac68c98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_detokenizer.xml:   0%|          | 0.00/2.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaeb269832641d5a5f4fced5dc5d375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0bb0501b2346309998a6395f06ef71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f61c18af239459a8a116829c6c9f4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9fdaecff884f6bbdcc6c472a3093a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_detokenizer.bin:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cef4363a4ff4bfc965dfa9110caf8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.xml:   0%|          | 0.00/4.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af732320d6c4c1c9115c5ffdd343e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9957ad188d2045c286bedaa5c8a26cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cd9a4c16e74b948024817f83cda634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_tokenizer.xml:   0%|          | 0.00/6.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d370d8535054df4b10c089cbafde2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e88c1ac48434fa1a9baf4f377e287bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.bin:   0%|          | 0.00/7.41G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd35093a54a4f34ac571835276bf34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_tokenizer.bin:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df83f047ea614190a814c7d4a518643c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub as hf_hub\n",
    "from pathlib import Path\n",
    "\n",
    "llm_model_id = \"OpenVINO/Phi-3-medium-4k-instruct-int4-ov\"\n",
    "llm_model_path = \"Phi-3-medium-4k-instruct-int4-ov\"\n",
    "\n",
    "if not Path(llm_model_path).exists():\n",
    "    hf_hub.snapshot_download(llm_model_id, local_dir=llm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09c9be",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48893aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/ethan/intel/Phi-3-workshop/openvino_env/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "Compiling the model to GPU ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openvino import OpenVINOLLM\n",
    "\n",
    "ov_config = {\n",
    "    \"PERFORMANCE_HINT\": \"LATENCY\",\n",
    "    \"NUM_STREAMS\": \"1\",\n",
    "    \"CACHE_DIR\": \"\",\n",
    "}\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|system|>{message.content}<|end|>\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|user|>{message.content}<|end|>\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|assistant|>{message.content}<|end|>\"\n",
    "\n",
    "    # ensure we start with a system prompt, insert blank if needed\n",
    "    if not prompt.startswith(\"<|system|>\"):\n",
    "        prompt = \"<|system|><|end|>\" + prompt\n",
    "\n",
    "    # add final assistant prompt\n",
    "    prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    return f\"<|system|><|end|><|user|>{completion}<|end|><|assistant|>\\n\"\n",
    "\n",
    "ov_llm = OpenVINOLLM(\n",
    "    model_id_or_path=llm_model_path,\n",
    "    context_window=3900,\n",
    "    max_new_tokens=1024,\n",
    "    model_kwargs={\"ov_config\": ov_config},\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    device_map=\"gpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c532ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stop_tokens = [\"<|endoftext|>\"]\n",
    "stop_tokens = ov_llm._tokenizer.convert_tokens_to_ids(stop_tokens)\n",
    "stop_tokens = [StopOnTokens(stop_tokens)]\n",
    "ov_llm._stopping_criteria = StoppingCriteriaList(stop_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f8935",
   "metadata": {},
   "source": [
    "### Export and initilize Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b7107c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_openvino import OpenVINOEmbedding\n",
    "\n",
    "embedding_model_path = \"bge-small-en-v1.5-ov\"\n",
    "ov_embedding = OpenVINOEmbedding(model_id_or_path=embedding_model_path, device=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12512b55",
   "metadata": {},
   "source": [
    "### Build tool of vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62080976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_example_en_path = Path(\"text_example_en.pdf\")\n",
    "Settings.embed_model = ov_embedding\n",
    "Settings.llm = ov_llm\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=text_example_en_path)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    transformations=[SentenceSplitter(chunk_size=200, chunk_overlap=40)],\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool(\n",
    "    index.as_query_engine(streaming=True),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"vector_search\",\n",
    "        description=\"Useful for searching for basic facts about Intel Xeon 6 processors\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ab46b",
   "metadata": {},
   "source": [
    "### Build tools of calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da2b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "\n",
    "\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "divide_tool = FunctionTool.from_defaults(fn=divide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebad8c6",
   "metadata": {},
   "source": [
    "### Create an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a573c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools([multiply_tool, divide_tool, vector_tool], llm=ov_llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf79029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ec006b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 05df0d98-ca32-4ac3-9bcb-bf5b1d1847f8. Step input: What's the maximum number of cores in an Intel Xeon 6 processor server with 4 sockets ?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: vector_quality\n",
      "Action Input: {'input': 'Intel Xeon 6 processor server with 4 sockets maximum cores'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: No such tool named `vector_quality`.\n",
      "\u001b[0m> Running step 3bf8025d-1bbe-4ca8-843c-00e7babf819c. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The tool `vector_quality` does not exist. I will use the `vector_search` tool to find information about Intel Xeon 6 processors.\n",
      "Action: vector_search\n",
      "Action Input: {'input': 'Intel Xeon 6 processor maximum cores with 4 sockets'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Based on the provided context information, the Intel Xeebon 6 processor with Efficient-cores can have up to 144 cores per socket. However, the information does not specify the maximum number of cores with 4 sockets. If we assume that the maximum number of cores per socket remains constant, then with 4 sockets, the maximum number of cores would be 144 cores/socket * apping 4 sockets = 576 cores. However, this is a theoretical maximum and actual configurations may vary.\n",
      "\u001b[0m> Running step 7e6a2535-4530-4511-a8c8-cbf843ee8710. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: Based on the information available, the Intel Xeon 6 processor with Efficient-cores can have up to 144 cores per socket. If we assume that the maximum number of cores per socket remains constant, then with apping 4 sockets, the maximum number of cores would be 144 cores/socket * 4 sockets = 576 cores. However, this is a theoretical maximum and actual configurations may vary.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What's the maximum number of cores in an Intel Xeon 6 processor server with 4 sockets ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "166e5732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 276b5e9e-4725-4a3a-abf7-f1ac0a0c4fdb. Step input: How about increase total number of sockets by 35% ? Go step by step, using a tool to do any math.\n",
      "\u001b[1;3;38;5;200mThought: I need to use the multiply tool to calculate the new total number of sockets.\n",
      "Action: multiply\n",
      "Action Input: {'a': 4, 'b': 1.35}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 5.4\n",
      "\u001b[0m> Running step a2881a50-1ab2-43a9-b265-013d835187bc. Step input: None\n",
      "\u001b[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
      "\u001b[0m> Running step ac386f4b-04a6-4acb-aa19-7d5fa5217468. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The output from the previous action is not in a valid format. I will round the result to the nearest whole number since the number of sockets must be a whole number. Then, I will use the multiply tool again to calculate the new total number of cores.\n",
      "Action: multiply\n",
      "Action Input: {'a': 144, 'b': 5}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 720\n",
      "\u001b[0m> Running step 71280572-594d-4002-a649-e4e60ade1d4a. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: With a 35% increase in the number of sockets, the new total number of sockets would be 5.4, which we round to 5. Therefore, the new total number of cores would be 720 cores (144 cores/socket * 5 sockets).\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"How about increase total number of sockets by 35% ? Go step by step, using a tool to do any math.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
