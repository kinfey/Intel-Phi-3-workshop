{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291f5de2",
   "metadata": {},
   "source": [
    "# Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ad753",
   "metadata": {},
   "source": [
    "### Download Phi-3-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub as hf_hub\n",
    "from pathlib import Path\n",
    "\n",
    "llm_model_id = \"OpenVINO/Phi-3-medium-4k-instruct-int4-ov\"\n",
    "llm_model_path = \"Phi-3-medium-4k-instruct-int4-ov\"\n",
    "\n",
    "if not Path(llm_model_path).exists():\n",
    "    hf_hub.snapshot_download(llm_model_id, local_dir=llm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09c9be",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48893aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openvino import OpenVINOLLM\n",
    "\n",
    "ov_config = {\n",
    "    \"PERFORMANCE_HINT\": \"LATENCY\",\n",
    "    \"NUM_STREAMS\": \"1\",\n",
    "    \"CACHE_DIR\": \"\",\n",
    "}\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|system|>{message.content}<|end|>\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|user|>{message.content}<|end|>\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|assistant|>{message.content}<|end|>\"\n",
    "\n",
    "    # ensure we start with a system prompt, insert blank if needed\n",
    "    if not prompt.startswith(\"<|system|>\"):\n",
    "        prompt = \"<|system|><|end|>\" + prompt\n",
    "\n",
    "    # add final assistant prompt\n",
    "    prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    return f\"<|system|><|end|><|user|>{completion}<|end|><|assistant|>\\n\"\n",
    "\n",
    "ov_llm = OpenVINOLLM(\n",
    "    model_id_or_path=llm_model_path,\n",
    "    context_window=3900,\n",
    "    max_new_tokens=1024,\n",
    "    model_kwargs={\"ov_config\": ov_config},\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    device_map=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c532ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stop_tokens = [\"<|endoftext|>\"]\n",
    "stop_tokens = ov_llm._tokenizer.convert_tokens_to_ids(stop_tokens)\n",
    "stop_tokens = [StopOnTokens(stop_tokens)]\n",
    "ov_llm._stopping_criteria = StoppingCriteriaList(stop_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f8935",
   "metadata": {},
   "source": [
    "### Export and initilize Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7107c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface_openvino import OpenVINOEmbedding\n",
    "\n",
    "embedding_model_path = \"bge-small-en-v1.5-ov\"\n",
    "ov_embedding = OpenVINOEmbedding(model_id_or_path=embedding_model_path, device=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12512b55",
   "metadata": {},
   "source": [
    "### Build tool of vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62080976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "\n",
    "Settings.embed_model = ov_embedding\n",
    "Settings.llm = ov_llm\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=text_example_en_path)\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "vector_tool = QueryEngineTool(\n",
    "    index.as_query_engine(streaming=True),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"vector_search\",\n",
    "        description=\"Useful for searching for basic facts about Intel Xeon 6 processors\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ab46b",
   "metadata": {},
   "source": [
    "### Build tools of calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "\n",
    "\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "divide_tool = FunctionTool.from_defaults(fn=divide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebad8c6",
   "metadata": {},
   "source": [
    "### Create an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a573c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools([multiply_tool, divide_tool, vector_tool], llm=ov_llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf79029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec006b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 84b4d6ed-32d8-41f8-843a-ef8370bead54. Step input: What's the maximum number of cores in an Intel Xeon 6 processor server with 4 sockets ?\n",
      "\u001b[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
      "\u001b[0m> Running step 61799bc6-f9d2-4d2b-b99a-68cc6182ade2. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: vector_search\n",
      "Action Input: {'input': 'maximum number of cores in an Intel Xeon 6 processor server with 4 sockets'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Up to 144 cores per socket in 1- or 2-socket configurations.\n",
      "\n",
      "To calculate the maximum number of cores in a server with 4 sockets using Intel Xeon 6 processors with E-cores, we can multiply the number of cores per socket by the number of sockets:\n",
      "\n",
      "144 cores/socket * 4 sockets = 576 cores\n",
      "\n",
      "Therefore, the maximum number of cores in an Intel Xeon 6 processor server with 4 sockets is 576 cores.\n",
      "\u001b[0m> Running step 13f99372-7af7-4e92-9b1c-b35f3d4b9b33. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer the question without using any more tools. I'll use the user's language to answer.\n",
      "Answer: The maximum number of cores in an Intel Xeon 6 processor server with 4 sockets is 576 cores.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What's the maximum number of cores in an Intel Xeon 6 processor server with 4 sockets ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e5732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 60fea1b1-8682-483a-bad4-e4626480d56a. Step input: How about increase total number of sockets by 14% ?\n",
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: To increase the total number of sockets by 14%, we first need to calculate the new number of sockets.\n",
      "\n",
      "New number of sockets = 4 sockets + (14% of 4 sockets)\n",
      "New number of sockets = 4 + (0.14 * 4)\n",
      "New number of sockets = 4 + 0.56\n",
      "New number of sockets = 4.56\n",
      "\n",
      "Since we cannot have a fraction of a socket, we will round up to the nearest whole number.\n",
      "\n",
      "New number of sockets = 5\n",
      "\n",
      "Now, we need to find the maximum number of cores for the Intel Xeon 6 processor with 5 sockets.\n",
      "\n",
      "The Intel Xeon 6 processor has 12 cores per socket.\n",
      "\n",
      "Maximum number of cores = 12 cores/socket * 5 sockets\n",
      "Maximum number of cores = 60 cores\n",
      "\n",
      "So, with an increase of 14% in the total number of sockets, the Intel Xeon 6 processor server will have a maximum of 60 cores.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"How about increase total number of sockets by 14% ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
