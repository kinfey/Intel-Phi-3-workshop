{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a24574f-810d-48f5-b6cf-92297cd21e30",
   "metadata": {},
   "source": [
    "# Lab-2-Chatbot&RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247ed1a-00a6-4005-b362-8dcb7c0eaa19",
   "metadata": {},
   "source": [
    "## 1. Basic Completion and Chat\n",
    "### Downlord Phi-3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d18203a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub as hf_hub\n",
    "from pathlib import Path\n",
    "\n",
    "llm_model_id = \"OpenVINO/Phi-3-mini-4k-instruct-int4-ov\"\n",
    "llm_model_path = \"Phi-3-mini-4k-instruct-ov\"\n",
    "\n",
    "if not Path(llm_model_path).exists():\n",
    "    hf_hub.snapshot_download(llm_model_id, local_dir=llm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928626d",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a75c915-d4fc-4016-89c6-dba61edea9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to GPU ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openvino import OpenVINOLLM\n",
    "\n",
    "ov_config = {\n",
    "    \"PERFORMANCE_HINT\": \"LATENCY\",\n",
    "    \"NUM_STREAMS\": \"1\",\n",
    "    \"CACHE_DIR\": \"\",\n",
    "}\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    return f\"<|system|><|end|><|user|>{completion}<|end|><|assistant|>\\n\"\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|system|>{message.content}<|end|>\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|user|>{message.content}<|end|>\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|assistant|>{message.content}<|end|>\"\n",
    "\n",
    "    # ensure we start with a system prompt, insert blank if needed\n",
    "    if not prompt.startswith(\"<|system|>\"):\n",
    "        prompt = \"<|system|><|end|>\" + prompt\n",
    "\n",
    "    # add final assistant prompt\n",
    "    prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "ov_llm = OpenVINOLLM(\n",
    "    model_id_or_path=llm_model_path,\n",
    "    context_window=3900,\n",
    "    max_new_tokens=1024,\n",
    "    model_kwargs={\"ov_config\": ov_config},\n",
    "    generate_kwargs={\"pad_token_id\": 32000, \"do_sample\": False, \"temperature\": None, \"top_p\": None},\n",
    "    stopping_ids=[32000],\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    device_map=\"gpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51204e94-b05c-4a86-b057-55614d017c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stop_tokens = [\"<|endoftext|>\"]\n",
    "stop_tokens = ov_llm._tokenizer.convert_tokens_to_ids(stop_tokens)\n",
    "stop_tokens = [StopOnTokens(stop_tokens)]\n",
    "ov_llm._stopping_criteria = StoppingCriteriaList(stop_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6719d-299c-4a11-9f0a-b2ef2d5db716",
   "metadata": {},
   "source": [
    "### Call complete with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9aec4ad-efd1-4a56-9343-e8d7cc61ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO, which stands for Open Visual Inference and Object Detection, is an open-source toolkit developed by Intel to optimize and deploy deep learning models on Intel hardware. It is designed to accelerate the inference of deep learning models on Intel CPUs and GPUs, making it easier for developers to deploy their models on Intel-based devices.\n",
      "\n",
      "OpenVINO provides a set of tools and libraries that help developers optimize their deep learning models for Intel hardware, including:\n",
      "\n",
      "1. Inference Engine: A high-performance, cross-platform, and extensible inference engine that supports various deep learning frameworks, such as TensorFlow, Caffe, and OpenCV.\n",
      "\n",
      "2. Inference Engine Optimizer: A tool that optimizes deep learning models for Intel hardware, reducing the model size and improving inference speed.\n",
      "\n",
      "3. Model Optimizer: A command-line tool that optimizes deep learning models for Intel hardware, reducing the model size and improving inference speed.\n",
      "rows\n",
      "4. Model Optimizer GUI: A graphical user interface that allows users to visualize and optimize deep learning models for Intel hardware.\n",
      "\n",
      "5. Model Optimizer API: An API that allows developers to integrate OpenVINO's optimization capabilities into their applications.\n",
      "\n",
      "OpenVINO's primary goal is to make it easier for developers to deploy their deep learning models on Intel hardware, enabling faster and more efficient inference. By optimizing deep learning models for Intel hardware, OpenVINO helps developers achieve better performance and lower latency, making it an essential tool for building intelligent applications on Intel-based devices."
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "response = ov_llm.stream_complete(\"What is OpenVINO ?\")\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06421eb-944b-4b2b-be84-cee0f54e151a",
   "metadata": {},
   "source": [
    "### Call chat with a list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f6fbc10-ec44-4522-8008-f164d61621fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse:\n",
      "\n",
      "In the heart of the city, where dreams intertwine,\n",
      "Kendrick stands tall, his voice a divine sign.\n",
      "With every word, he paints a vivid scene,\n",
      "A story of struggle, of love, of what it means to be.\n",
      "\n",
      "His rhymes like brushstrokes, bold and free,\n",
      "A masterpiece of words, for all to see.\n",
      "From the streets to the stage, his journey's been long,\n",
      "But his passion for music, it's never gone.\n",
      "\n",
      "Thrrows of verses, like pearls on a string,\n",
      "Each one a gem, a message to bring.\n",
      "From the depths of his soul, they spring,\n",
      "A testament to the power of his lyrical swing.\n",
      "\n",
      "His music, a beacon, in the darkest night,\n",
      "Guiding lost souls, giving them light.\n",
      "With every chorus, every hook,\n",
      "He's a force of nature, a lyrical crook.\n",
      "\n",
      "So here's to Kendrick, the wordsmith supreme,\n",
      "A poet, a rapper, a dreamer's dream.\n",
      "His verses, a mirror to our own,\n",
      "A reflection of the world, in his own tone.\n",
      "\n",
      "For in his music, we find a truth,\n",
      "A voice for the voiceless, a beacon of youth.\n",
      "So let's raise a toast, to Kendrick's verse,\n",
      "A gift to us all, a blessing to rehearse."
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are Kendrick.\"),\n",
    "    ChatMessage(role=\"user\", content=\"Write a verse.\"),\n",
    "]\n",
    "\n",
    "response = ov_llm.stream_chat(messages)\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c6cdf6-4358-43aa-a380-2c315f36c228",
   "metadata": {},
   "source": [
    "## 2. Basic RAG (Vector Search, Summarization)\n",
    "### Export Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "536ea7ff-033b-4a53-ba2c-6169c69e47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "embedding_model_path = \"bge-small-en-v1.5-ov\"\n",
    "\n",
    "if not Path(embedding_model_path).exists():\n",
    "    !optimum-cli export openvino --model {embedding_model_id} --task feature-extraction {embedding_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f82691",
   "metadata": {},
   "source": [
    "### Initialize Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7a768ac-b701-4722-8d88-f1d2561feb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_openvino import OpenVINOEmbedding\n",
    "\n",
    "ov_embedding = OpenVINOEmbedding(model_id_or_path=embedding_model_path, device=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ace02-5bec-4815-a709-0b6423497516",
   "metadata": {},
   "source": [
    "### Basic RAG (Vector Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed13be16-0364-445e-acc9-d0b466411d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "Settings.embed_model = ov_embedding\n",
    "Settings.llm = ov_llm\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"Product Brief.txt\"]\n",
    ")\n",
    "documents = reader.load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b492fb4-7667-492b-b8f8-0048fc2b592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of cores per socket in an Intel Xeon 6 processor is up to 144 cores."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\"what's the maximum number of cores per socket in an Intel Xeon 6 processor\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1931c07-e568-4586-b5e7-885f450b9082",
   "metadata": {},
   "source": [
    "### Basic RAG (Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edcd0e55-3513-49d2-b974-6e0374a8be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "\n",
    "summary_index = SummaryIndex.from_documents(documents)\n",
    "summary_engine = summary_index.as_query_engine(streaming=True, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25d580db-0a57-484a-b558-414b51b807a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Intel® Xeon® 6 processors with Efficient-cores (E-cores) are designed to meet the demands of network and edge scalability, offering high performance per watt and up to 144 cores per socket. These processors provide greater parallel processing for workload throughput, maximizing value. They are built with integrated accelerators to enhance efficiency and productivity for network and security workloads. The E-cores support next-gen security capabilities, such as deep packet inspection and zero trust network, and are equipped with Intel® Trust Domain Extensions (Intel® TDX) and Intel® Software Guard Extensions (Intel® SGX) for secure in-flight applications and data. The processors also offer longevity, simplified designs, and longer deployments, with support from Intel® Ethernet, GPU, IPU, and an ecosystem of optimized software and tools. These features make Intel® Xeon® 6 processors with E-cores ideal for software engineers and network architects working on edge solutions, 5G networks, and edge-to-cloud infrastructure."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\"Can you summerize this document ?\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b04a03-7906-4fa0-ba1d-1419bd0c94d6",
   "metadata": {},
   "source": [
    "## 3. Advanced RAG (Routing)\n",
    "### Build a Router that can choose whether to do vector search or summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a94d6747-8c41-4d05-b011-082b554088f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "vector_tool = QueryEngineTool(\n",
    "    index.as_query_engine(streaming=True),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"vector_search\",\n",
    "        description=\"Useful for searching for basic facts about Intel Xeon 6 processors\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool(\n",
    "    index.as_query_engine(streaming=True, response_mode=\"tree_summarize\"),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"summary\",\n",
    "        description=\"Useful for summarizing an entire document of Intel Xeon 6 processors\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a86ec58-15e1-4e1e-b761-7e3ea7bc9b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The first choice is most relevant because it pertains to providing basic facts about Intel Xeon 6 processors, which would likely include specifications such as the maximum number of cores per socket..\n",
      "\u001b[0mThe maximum number of cores per socket in an Intel Xeon 6 processor is up to 144 cores."
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "\n",
    "query_engine = RouterQueryEngine.from_defaults(\n",
    "    [vector_tool, summary_tool], select_multi=False, verbose=True, llm=ov_llm\n",
    ")\n",
    "\n",
    "streaming_response = query_engine.query(\n",
    "    \"what's the maximum number of cores per socket in an Intel Xeon 6 processor\"\n",
    ")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4526ea68-d594-4c57-9a42-b028e315aa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question 'Can you summarize this document?' directly relates to the ability to provide a concise overview of an entire document, which aligns with choice (2)..\n",
      "\u001b[0mThe Intel® Xeon® 6 processors with Efficient-cores (E-cores) are designed to meet the demands of network and edge scalability, offering high performance per watt and up to 144 cores per socket. These processors provide greater parallel processing capabilities, enhancing workload throughput and value. They are optimized for networking, edge, and security workloads, with integrated accelerators for improved efficiency and productivity. The E-cores support next-gen security features like deep packet inspection and zero trust networks, and utilize confidential computing technologies to secure data. The processors offer longevity and simplified designs, with PCH-less boot and compatibility with P-core processors, ensuring businesses can capitalize on their investments. Overall, Intel® Xeon® 6 processors with E-cores are ideal for software engineers and network architects seeking to boost edge solutions, enhance security, and improve sustainability efforts."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"Can you summerize this document ?\"\n",
    ")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f757fe7",
   "metadata": {},
   "source": [
    "## 4. Agentic RAG\n",
    "\n",
    "### Build tools of calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a444187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "\n",
    "\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "divide_tool = FunctionTool.from_defaults(fn=divide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b868cc",
   "metadata": {},
   "source": [
    "### Create an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76419c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools([multiply_tool, divide_tool, vector_tool], llm=ov_llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a86daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 42aef0c2-4b73-4285-8af1-92d7c4408bb5. Step input: What's the maximum number of cores of 6 sockets of Intel Xeon 6 processors ? Go step by step, using a tool to do any math.\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: vector_search\n",
      "Action Input: {'input': 'maximum number of cores in Intel Xeon 6 processors'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: The maximum number of cores in Intel Xeon 6 processors is up to 144 cores per socket.\n",
      "\u001b[0m> Running step ad29b7e9-e8df-496a-8a4d-f2ffb91b054b. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: The maximum number of cores for an Intel Xeon 6 processor with 6 sockets is 864 cores (144 cores per socket multiplied by 6 sockets).\n",
      "\n",
      "To calculate this, we can use the multiply tool:\n",
      "\n",
      "Thought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Action: multiply\n",
      "Action Input: {'a': 144, 'b': 6}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 864\n",
      "\u001b[0m> Running step 320bd891-594a-4f1a-926f-2caebe9b866a. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: The maximum number of cores for an Intel Xeon 6 processor with 6 sockets is 864 cores.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What's the maximum number of cores of 6 sockets of Intel Xeon 6 processors ? Go step by step, using a tool to do any math.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de0af422",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
